{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2e6bc-798e-4534-80ca-2181e97c95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "BUCKET_URI = f\"gs://sid-vertex-mlops\"\n",
    "TRAIN_COMPUTE = \"n1-standard-4\"\n",
    "TRAIN_VERSION = \"xgboost-cpu.1-1\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest\"\n",
    "\n",
    "machine_spec = {\"machine_type\": TRAIN_COMPUTE, \"accelerator_count\": 0}\n",
    "\n",
    "aiplatform.init(project=\"udemy-mlops\", staging_bucket=BUCKET_URI)\n",
    "\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "setup_py = \"\"\"\n",
    "import setuptools\n",
    "\n",
    "setuptools.setup(\n",
    "    install_requires=[\n",
    "        'cloudml-hypertune',\n",
    "        'gcsfs',\n",
    "        'category_encoders==2.6.1',\n",
    "        'imbalanced-learn==0.11.0',\n",
    "        'scikit-learn>=0.24.0',\n",
    "    ],\n",
    "    packages=setuptools.find_packages()\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'cloudml-hypertune','gcsfs','category_encoders','imbalanced-learn',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080bd3eb-4217-4aff-8a4f-333260665869",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders import HashingEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from google.cloud import storage\n",
    "import hypertune\n",
    "import argparse\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(\"sid-kubeflow-v1\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_estimators\", dest=\"n_estimators\",default=20, type=int, help=\"Number of estimators\")\n",
    "parser.add_argument(\"--learning_rate\", dest=\"learning_rate\",default=0.2, type=float, help=\"Learning Rate\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \n",
    "    df = df.drop(columns=['car', 'toCoupon_GEQ5min', 'direction_opp'])\n",
    "    df = df.fillna(df.mode().iloc[0])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df_dummy = df.copy()\n",
    "    age_list = []\n",
    "    for i in df['age']:\n",
    "        if i == 'below21':\n",
    "            age = '<21'\n",
    "        elif i in ['21', '26']:\n",
    "            age = '21-30'\n",
    "        elif i in ['31', '36']:\n",
    "            age = '31-40'\n",
    "        elif i in ['41', '46']:\n",
    "            age = '41-50'\n",
    "        else:\n",
    "            age = '>50'\n",
    "        age_list.append(age)\n",
    "    df_dummy['age'] = age_list\n",
    "\n",
    "    df_dummy['passanger_destination'] = df_dummy['passanger'].astype(str) + '-' + df_dummy['destination'].astype(str)\n",
    "    df_dummy['marital_hasChildren'] = df_dummy['maritalStatus'].astype(str) + '-' + df_dummy['has_children'].astype(str)\n",
    "    df_dummy['temperature_weather'] = df_dummy['temperature'].astype(str) + '-' + df_dummy['weather'].astype(str)\n",
    "    df_dummy = df_dummy.drop(columns=['passanger', 'destination', 'maritalStatus', 'has_children', 'temperature','weather', 'Y'])\n",
    "\n",
    "    df_dummy = pd.concat([df_dummy, df['Y']], axis = 1)\n",
    "    df_dummy = df_dummy.drop(columns=['gender', 'RestaurantLessThan20'])\n",
    "    df_le = df_dummy.replace({\n",
    "        'expiration':{'2h': 0, '1d' : 1},\n",
    "        'age':{'<21': 0, '21-30': 1, '31-40': 2, '41-50': 3, '>50': 4},\n",
    "        'education':{'Some High School': 0, 'High School Graduate': 1, 'Some college - no degree': 2,\n",
    "                     'Associates degree': 3, 'Bachelors degree': 4, 'Graduate degree (Masters or Doctorate)': 5},\n",
    "        'Bar':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4},\n",
    "        'CoffeeHouse':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4}, \n",
    "        'CarryAway':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4}, \n",
    "        'Restaurant20To50':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4},\n",
    "        'income':{'Less than $12500':0, '$12500 - $24999':1, '$25000 - $37499':2, '$37500 - $49999':3,\n",
    "                  '$50000 - $62499':4, '$62500 - $74999':5, '$75000 - $87499':6, '$87500 - $99999':7,\n",
    "                  '$100000 or More':8},\n",
    "        'time':{'7AM':0, '10AM':1, '2PM':2, '6PM':3, '10PM':4}\n",
    "    })\n",
    "\n",
    "    x = df_le.drop('Y', axis=1)\n",
    "    y = df_le.Y\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def train_model(x_train, y_train, learning_rate, n_estimators, max_depth=None):\n",
    "    model = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, x_test, y_test, x_sm_train_hashing, y_sm_train):\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_proba = model.predict_proba(x_test)\n",
    "    y_pred_train = model.predict(x_sm_train_hashing)\n",
    "    y_pred_train_proba = model.predict_proba(x_sm_train_hashing)\n",
    "    \n",
    "    return accuracy_score(y_test, y_pred),precision_score(y_test, y_pred)\n",
    "\n",
    "def encode_features(x, n_components=27):\n",
    "    hashing_ros_enc = HashingEncoder(cols=['passanger_destination', 'marital_hasChildren', 'occupation', 'coupon',\n",
    "                                           'temperature_weather'], n_components=n_components).fit(x)\n",
    "    x_test_hashing = hashing_ros_enc.transform(x.reset_index(drop=True))\n",
    "    return x_test_hashing\n",
    "\n",
    "def oversample_data(x_train_hashing, y_train):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_sm_train_hashing, y_sm_train = sm.fit_resample(x_train_hashing, y_train)\n",
    "    return x_sm_train_hashing, y_sm_train\n",
    "\n",
    "def get_score(model, x, y, x_test, y_test):\n",
    "    model.fit(x, y)\n",
    "    y_pred = model.predict_proba(x_test)[:, 1]\n",
    "    score = roc_auc_score(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "input_file = \"gs://sid-kubeflow-v1/coupon-recommendation/in-vehicle-coupon-recommendation.csv\"\n",
    "df = load_data(input_file)\n",
    "\n",
    "\n",
    "n_estimators = args.n_estimators\n",
    "learning_rate = args.learning_rate\n",
    "\n",
    "x, y = preprocess_data(df)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "x_train.fillna(x_train.mode().iloc[0], inplace=True)\n",
    "x_test.fillna(x_train.mode().iloc[0], inplace=True)\n",
    "\n",
    "model_name = 'xgboost'\n",
    "print(\"Training and evaluating\", model_name, \"model:\")\n",
    "x_train_hashing = encode_features(x_train)\n",
    "x_test_hashing = encode_features(x_test)\n",
    "x_sm_train_hashing, y_sm_train = oversample_data(x_train_hashing, y_train)\n",
    "\n",
    "pipeline = train_model(x_sm_train_hashing, y_sm_train, learning_rate, n_estimators, max_depth=None)\n",
    "\n",
    "accuracy, precision = evaluate_model(pipeline, x_test_hashing, y_test, x_sm_train_hashing, y_sm_train)\n",
    "\n",
    "hpt = hypertune.HyperTune()\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag='accuracy',\n",
    "    metric_value=accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec6f04-ebb9-410e-ad8d-02018b3f0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/xgboost_classification.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3f3d6-7746-47c3-ad9e-369d54d93d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISK_TYPE = \"pd-ssd\"\n",
    "DISK_SIZE = 200\n",
    "\n",
    "disk_spec = {\"boot_disk_type\": DISK_TYPE, \"boot_disk_size_gb\": DISK_SIZE}\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"disk_spec\": disk_spec,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [BUCKET_URI + \"/xgboost_classification.tar.gz\"],\n",
    "            \"python_module\": \"trainer.task\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=\"xgboost_hpt_tuning\",\n",
    "    worker_pool_specs=worker_pool_spec\n",
    ")\n",
    "\n",
    "hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=\"xgboost_hpt_job\",\n",
    "    custom_job=job,\n",
    "    metric_spec={\n",
    "        \"accuracy\": \"maximize\"\n",
    "    },\n",
    "    parameter_spec={\n",
    "        \"n_estimators\": hpt.IntegerParameterSpec(min=30,max=40,scale=\"linear\"),\n",
    "        \"learning_rate\": hpt.DoubleParameterSpec(min=0.2,max=0.5,scale=\"linear\")\n",
    "    },\n",
    "    search_algorithm=None,\n",
    "    max_trial_count=2,\n",
    "    parallel_trial_count=2\n",
    ")\n",
    "\n",
    "hpt_job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
